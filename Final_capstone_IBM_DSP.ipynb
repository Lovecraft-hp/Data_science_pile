{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 - Libraries and Geoinfo file upload\n",
    "\n",
    "## Available data analysis NYC information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first import needed libraries\n",
    "import numpy as np # numpy arrays\n",
    "import pandas as pd # dataframes and many many other things\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "import json # work with json files\n",
    "import requests #make requests to urls\n",
    "\n",
    "import urllib #open urls\n",
    "\n",
    "! pip install folium\n",
    "import folium\n",
    "\n",
    "# installed shapely library from a wheel file - https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "# instruction to install library into conda using cmd - https://www.geeksforgeeks.org/python-add-packages-to-anaconda-environment/\n",
    "!pip install shapely\n",
    "import shapely\n",
    "import shapely.wkt\n",
    "\n",
    "from shapely import geometry\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "import ast # for transforming strings to the list/tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we take data related to the New-york city (NYC from now on) Neighborhood tabulation areas (NTAs) <br>\n",
    "For this I've searched the internet and got to the valuable site, and this directory https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-nynta.page <br>\n",
    "https://data.cityofnewyork.us/City-Government/New-York-City-Population-By-Neighborhood-Tabulatio/swpk-hqdp/data <br>\n",
    "here we've found population amounts by NTAs, so we definetely can calculate the population density based on this information. <br>\n",
    "and income information by NTA 'http://www.city-data.com/nbmaps/neigh-New-York-New-York.html'\n",
    "'https://statisticalatlas.com/place/New-York/New-York/Household-Income#figure/neighborhood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_geoinfo='http://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/nynta/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=geojson'\n",
    "with urllib.request.urlopen(url_geoinfo) as ref_geoinfo:\n",
    "    json_geoinfo=json.load(ref_geoinfo)\n",
    "\n",
    "json_geoinfo.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - preparation of open source New-york city data\n",
    "\n",
    "So, we've stumbled upon the infromation regarding the shapes of the NYC NTAs <br>\n",
    "Luckily, I've found __shapely__ library to work with the shape objects, understand if some venue (ll point) is inside a shape, <br> \n",
    "calculate area sizes and find centroids of the NTAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after research of the json file's structure, let's see one element example\n",
    "print(json_geoinfo['features'][0].keys())\n",
    "print(json_geoinfo['features'][0]['properties'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a dataframe that holds all the shapes and desired infromation regarding the shapes\n",
    "shape_tlist=[]\n",
    "shape_clist=[]\n",
    "shape_ntalist=[]\n",
    "shape_arealist=[]\n",
    "shape_countlist=[]\n",
    "for i,elem in enumerate(json_geoinfo['features']):\n",
    "    shape_type=elem['geometry']['type']\n",
    "    shape_tlist.append(shape_type)\n",
    "    shape_ntalist.append(elem['properties']['NTACode'])\n",
    "    shape_arealist.append(elem['properties']['Shape__Area']/10000000)\n",
    "    #print(i,shape_type)\n",
    "    shape_coord=elem['geometry']['coordinates'];\n",
    "    if shape_type=='Polygon':\n",
    "        shape_obj=geometry.Polygon(shape_coord[0])\n",
    "        i=1\n",
    "    elif shape_type=='MultiPolygon':\n",
    "        #print(shape_coord[1])\n",
    "        shape_multi=[]\n",
    "        i=0\n",
    "        for shape_elem in shape_coord:\n",
    "            for shape_detail in shape_elem:\n",
    "                i+=1\n",
    "                shape_multi.append(geometry.Polygon(shape_detail))\n",
    "        shape_obj=geometry.MultiPolygon(shape_multi)\n",
    "    else:\n",
    "        shape_obj='NA'\n",
    "    shape_clist.append(shape_obj)\n",
    "    shape_countlist.append(i)\n",
    "df_NTA=pd.DataFrame(zip(shape_ntalist,shape_tlist, shape_countlist,shape_clist,shape_arealist), columns=['NTA Code','Shape type','Poly Count','geometry','Shape area size, km'])\n",
    "df_NTA.set_index('NTA Code',inplace=True)\n",
    "df_NTA.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NTA['geometry'].loc['BX99']\n",
    "#that's how a shapely multipolygon object is displayed - in comparison with the map, this is the parks in bronx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes our first task - to extract the infromation regarding how the neighborhoods look like\n",
    "Next, we're going to calculate based on the shapes, what is the square area size, what are the centroids of the noted shapes, and what are the max distance from every shape point to it's centroid (we're gonna need it when we'll extract the data from FourSquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also there is an error between built-in shape area size and polygon area simply because the Earth isn't flat, so you need to keep in mind of such inaccuracy and how it can affect your calculation\n",
    "df_NTA['Polygon area, km']=df_NTA['geometry'].apply(lambda x:x.area*10000)\n",
    "\n",
    "#second - let's point out centroids (or a list of centroids for polygons)\n",
    "def multi_centroid(multi_shape):\n",
    "    cent_list=[]\n",
    "    for poly in multi_shape:\n",
    "        cent_list.append(poly.centroid.coords[0])\n",
    "    return cent_list\n",
    "\n",
    "df_NTA.loc[df_NTA['Poly Count']>1,'Shape centroid']=df_NTA.loc[df_NTA['Poly Count']>1,'geometry'].apply(lambda x:multi_centroid(x))\n",
    "df_NTA.loc[df_NTA['Poly Count']==1,'Shape centroid']=df_NTA.loc[df_NTA['Poly Count']==1,'geometry'].apply(lambda x:x.centroid.coords[0])\n",
    "\n",
    "#third - let's calculate the the farthest point inside each polygon from the centroid, and that we did with the notion of the earth parameters, even if we've made the algorithm a little longer by it\n",
    "\n",
    "def farthest_determinator(poly_shape):\n",
    "    cent=poly_shape.centroid\n",
    "    max=0\n",
    "    for ll in poly_shape.exterior.coords:\n",
    "        if max<geopy.distance.geodesic(ll, (cent.x,cent.y)).km:\n",
    "            max=geopy.distance.geodesic(ll, (cent.x,cent.y)).km\n",
    "    return max\n",
    "\n",
    "def multi_far(multi_shape):\n",
    "    max_list=[]\n",
    "    for poly in multi_shape:\n",
    "        max_list.append(farthest_determinator(poly))\n",
    "    return max_list\n",
    "\n",
    "df_NTA.loc[df_NTA['Poly Count']>1,'Max radius, km']=df_NTA.loc[df_NTA['Poly Count']>1,'geometry'].apply(lambda x:multi_far(x))\n",
    "df_NTA.loc[df_NTA['Poly Count']==1,'Max radius, km']=df_NTA.loc[df_NTA['Poly Count']==1,'geometry'].apply(lambda x:farthest_determinator(x))\n",
    "\n",
    "df_NTA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we're taking the information regarding the population size by NTA's from another source\n",
    "Unfortunately, the latest infrormaton available is 2010, but We'll try to look for additional infromation later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_population='https://data.cityofnewyork.us/api/views/swpk-hqdp/rows.csv'\n",
    "df_population=pd.read_csv(url_population)\n",
    "df_population=df_population.loc[df_population['Year']==2010]\n",
    "df_population.reset_index(drop=True,inplace=True)\n",
    "df_population.set_index('NTA Code',inplace=True)\n",
    "\n",
    "df_merged=df_population.join(df_NTA,on='NTA Code')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_borough_pop=df_merged[['Shape area size, km','Polygon area, km','Borough']].groupby(by='Borough').sum()\n",
    "# Polygon area had been calculated in order to check the reasonableness of the default built-in shape area size information, however the differences are caused by\n",
    "# the fact that you cannot just calculate the area size of 2D object, because the Earth is not flat\n",
    "df_borough_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_borough_pop.drop('Polygon area, km',axis=1,inplace=True)\n",
    "df_merged.drop('Polygon area, km',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the next step, we're gonna look for additional information sourses and check the provided data quality based on external information\n",
    "Like - the borough area sizes from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we double check the data with aggregated Borough information available (couldn't find information for every NTA)\n",
    "from bs4 import BeautifulSoup\n",
    "url_wiki='https://en.wikipedia.org/wiki/Boroughs_of_New_York_City'\n",
    "wiki_html=urllib.request.urlopen(url_wiki).read()\n",
    "wiki_soup=BeautifulSoup(wiki_html,'html.parser')\n",
    "#wiki_soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup_table = wiki_soup.find('table',{'class':'wikitable sortable'})\n",
    "#so then, after we locate the table, we only take that relevant part, and find every text line by searching 'td'\n",
    "soup_links=soup_table.findAll('td')\n",
    "columns=['Borough', 'County', 'Population','GDP US bln','GDP per capita','Area sq ml','Area sq km','Density sq ml','Density sq km']\n",
    "NYC_borough_stat=pd.DataFrame(columns=columns)\n",
    "temp_dict={}\n",
    "for n,line in enumerate(soup_links):\n",
    "#basically, every element in 'soup_links' is every cell in the table, and as we know there are 9 elements in every row, so we are configuring this algorithm to\n",
    "#re-write a dictionary 'temp_dict' on every 9-rd iteration and append it to the created dataframe 'postal_codes'\n",
    "    curr=line.get_text().strip('\\n') # at this line - we're getting only relevant text without any html symbols and other things, also we're stripping the next line symbol\n",
    "#this if is served for multi-purpose 1. if n%9==0 then the we've got the full temp_dict recorded that we need to record\n",
    "# however if n==0, then we're at the start of the loop, and we don't have anything to write yet\n",
    "# also it is given at the task that we need to delete all unassigned lists\n",
    "    if (n%9==0) and (n!=0) and (temp_dict['Borough']!='Not assigned'):\n",
    "# next if is to see when the Borough already had been recorded, so if the loc method with condition inside shows that it's empty - then we need to write a new line\n",
    "        if NYC_borough_stat.loc[NYC_borough_stat['Borough']==temp_dict['Borough']].empty:\n",
    "            NYC_borough_stat=NYC_borough_stat.append(temp_dict,ignore_index=True)\n",
    "    temp_dict[columns[n%9]]=curr\n",
    "    \n",
    "# if a neighbourhood not assigned, then it must be replaced\n",
    "NYC_borough_stat['Borough'].loc[0]='Bronx'\n",
    "NYC_borough_stat.set_index('Borough',inplace=True)\n",
    "NYC_borough_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# it seems like have some differences, and in order to minimize them, \n",
    "# let's create a normalizing square area coefficients for each borough and then apply it to the NTA information\n",
    "df_borough_pop['stat area sq km']=NYC_borough_stat['Area sq km']\n",
    "df_borough_pop['area_coef']=df_borough_pop.apply(lambda x: float(x['stat area sq km'])/float(x['Shape area size, km']), axis=1)\n",
    "df_borough_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_merged.drop('area_coef',axis=1,inplace=True)\n",
    "except:\n",
    "    print('no area_coef column present_yet')\n",
    "df_merged=df_merged.join(df_borough_pop['area_coef'],on='Borough')\n",
    "df_merged['Area_normalized, sq km']=df_merged.apply(lambda x: float(x['area_coef'])*x['Shape area size, km'], axis=1)\n",
    "df_merged.drop(['area_coef','Shape area size, km'],axis=1,inplace=True)\n",
    "df_merged['Population_density']=df_merged.apply(lambda x: int(x['Population'])/float(x['Area_normalized, sq km']),axis=1)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's see aditional sourse of information regarding the income, population and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "zip_url='https://s3.amazonaws.com/geoda/data/nycnhood_acs.zip'\n",
    "zip_bytes=urllib.request.urlopen(zip_url).read()\n",
    "#this is the zip file we need to open and iterate through it's contents\n",
    "with zipfile.ZipFile(io.BytesIO(zip_bytes)) as my_zip_file:\n",
    "    zinfo=my_zip_file.infolist()\n",
    "    znames=my_zip_file.namelist()\n",
    "znames\n",
    "# so, now we can see, that we need to open the dbf file inside the zip file and shape object to see if they have more accurate data\n",
    "# 2 files - 'NYC_Nhood ACS2008_12.dbf', 'NYC_Nhood ACS2008_12.shp', number 0 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpledbf\n",
    "!pip install pyshp\n",
    "from simpledbf import Dbf5\n",
    "import shapefile\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(zip_bytes)) as my_zip_file:\n",
    "    dbf_ref=my_zip_file.extract(zinfo[0])\n",
    "    dbf=Dbf5(dbf_ref)\n",
    "    df_dbf = dbf.to_dataframe()\n",
    "    shp_ref=my_zip_file.extract(zinfo[5])\n",
    "    shp = shapefile.Reader(shp_ref)\n",
    "print(shp.fields)\n",
    "df_dbf.head()\n",
    "#zinfo[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_shapelist=[]\n",
    "alt_shape_ntas=[]\n",
    "alt_shape_boronames=[]\n",
    "alt_shape_type=[]\n",
    "alt_shape_count=[]\n",
    "for feature in shp.shapeRecords():\n",
    "    alt_shape_ntas.append(feature.record[-7]) # this is the information file with details - however all of them we took from dbf file, here we're linking only the NTA code\"\n",
    "    alt_shape_boronames.append(feature.record[-9])\n",
    "    shape_obj=shapely.geometry.shape(feature.shape)\n",
    "    alt_shapelist.append(shape_obj)\n",
    "    i=1\n",
    "    if isinstance(shape_obj,shapely.geometry.polygon.Polygon):\n",
    "        i=1\n",
    "        shp_type='Polygon'\n",
    "    else:\n",
    "        for x in shape_obj:\n",
    "            i+=1\n",
    "        shp_type='MultiPolygon'\n",
    "    alt_shape_type.append(shp_type)\n",
    "    alt_shape_count.append(i)\n",
    "df_alt_NTA=pd.DataFrame(zip(alt_shape_ntas,alt_shape_boronames, alt_shapelist,alt_shape_type,alt_shape_count), columns=['NTA Code','Borough','geometry','Shape type','Poly Count'])\n",
    "df_alt_NTA.set_index('NTA Code',inplace=True)\n",
    "df_alt_NTA['alt Polygon area, km']=df_alt_NTA['geometry'].apply(lambda x:x.area*10000)\n",
    "df_alt_NTA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_borough_alt=df_alt_NTA[['alt Polygon area, km','Borough']].groupby(by='Borough').sum()\n",
    "df_borough_pop2=df_borough_pop.join(df_borough_alt)\n",
    "df_borough_pop2\n",
    "#df_dbf=df_dbf.join(df_alt_NTA,on='ntacode')\n",
    "#df_dbf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know that these polygons are somehow different, however from the polygon area size accuracy point of view, they are very much alike, so let's keep the old one for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_alt_info='https://geodacenter.github.io/data-and-lab//NYC-Nhood-ACS-2008-12/'\n",
    "alt_html=urllib.request.urlopen(url_alt_info).read()\n",
    "alt_soup=BeautifulSoup(alt_html,'html.parser')\n",
    "#print(alt_soup.prettify())\n",
    "\n",
    "soup_table = alt_soup.find('table')\n",
    "#so then, after we locate the table, we only take that relevant part, and find every text line by searching 'td'\n",
    "soup_links=soup_table.findAll('td')\n",
    "columns=['Variable','Meaning']\n",
    "alt_variables=pd.DataFrame(columns=columns)\n",
    "temp_dict={}\n",
    "for n,line in enumerate(soup_links):\n",
    "#basically, every element in 'soup_links' is every cell in the table, and as we know there are 9 elements in every row, so we are configuring this algorithm to\n",
    "#re-write a dictionary 'temp_dict' on every 9-rd iteration and append it to the created dataframe 'postal_codes'\n",
    "    curr=line.get_text().strip('\\n') # at this line - we're getting only relevant text without any html symbols and other things, also we're stripping the next line symbol\n",
    "#this if is served for multi-purpose 1. if n%9==0 then the we've got the full temp_dict recorded that we need to record\n",
    "# however if n==0, then we're at the start of the loop, and we don't have anything to write yet\n",
    "# also it is given at the task that we need to delete all unassigned lists\n",
    "    if (n%2==0) and (n!=0):\n",
    "# next if is to see when the Borough already had been recorded, so if the loc method with condition inside shows that it's empty - then we need to write a new line\n",
    "        if alt_variables.loc[alt_variables['Variable']==temp_dict['Variable']].empty:\n",
    "            alt_variables=alt_variables.append(temp_dict,ignore_index=True)\n",
    "    temp_dict[columns[n%2]]=curr\n",
    "alt_variables=alt_variables.loc[alt_variables['Variable'].isin(['medianinco',\n",
    "                                                                 'popinlabou',\n",
    "                                                                 'ntacode',\n",
    "                                                                 'boroname',\n",
    "                                                                 'popdty',\n",
    "                                                                 'medianage',\n",
    "                                                                 'poptot'])]\n",
    "alt_variables=alt_variables.append(pd.DataFrame([['UEMPRATE','Some coeffient, that is used for some relative data like median age and income']],columns=['Variable','Meaning']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbf2=df_dbf.loc[:,alt_variables['Variable'].tolist()]\n",
    "df_dbf2.columns=['poptot','popinlabou', 'Borough', 'popdty','NTA Code','medianinco', 'medianage','UEMPRATE']\n",
    "df_dbf2.sort_values(by='NTA Code',inplace=True)\n",
    "def mile_to_km(x):\n",
    "    if x=='NA':\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return float(x)/2.58999\n",
    "def floater(x):\n",
    "    if x=='NA':\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return float(x)\n",
    "    \n",
    "df_dbf2['popdty, per sq km']=df_dbf2.loc[:,'popdty'].apply(mile_to_km)\n",
    "df_dbf2['medianage']=df_dbf2.loc[:,'medianage'].apply(floater)\n",
    "df_dbf2['medianinco']=df_dbf2.loc[:,'medianinco'].apply(floater)\n",
    "df_dbf2.drop('popdty',axis=1,inplace=True)\n",
    "df_dbf2.set_index('NTA Code',inplace=True)\n",
    "print(df_dbf2.sum())\n",
    "df_dbf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we see an error with the data presented - it obviously has faulty information - population density needs to be divided by 10 at least, median age is somewhere skyhigh and the median income is also showing inconsistency\n",
    "For the purpose of this test, let's assume that this data (and with gini and other parameters are somehow hinting this) had been just normalized for the purpose of some model creation. <br>\n",
    "And our task is to make it readable and to denormalize it - and get the estimated readable amounts that looks like median income and median age.<br>\n",
    "So, first, let's calculate the weighted average amounts and the min and max values to see the situation <br>\n",
    "Second, we're going to compare it with the median values for age and income for NYC and it's boroughs, and set up the linear transformation, that will change the current values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_income(x):\n",
    "    if x['medianinco']=='NA' or x['poptot']=='NA' or x['poptot']==0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return float(x['medianinco'])*float(x['poptot'])/8199221\n",
    "\n",
    "def weighted_age(x):\n",
    "    if x['medianage']=='NA' or x['poptot']=='NA' or x['poptot']==0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return float(x['medianage'])*float(x['poptot'])/8199221\n",
    "\n",
    "def median_income(x):\n",
    "    if x['medianinco']=='NA' or x['poptot']=='NA' or x['poptot']==0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return float(x['medianinco'])\n",
    "\n",
    "def median_age(x):\n",
    "    if x['medianage']=='NA' or x['poptot']=='NA' or x['poptot']==0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return float(x['medianage'])\n",
    "\n",
    "def labour_calc(x):\n",
    "    if x['popinlabou']=='NA' or x['poptot']=='NA' or x['poptot']==0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return float(x['popinlabou'])/float(x['poptot'])\n",
    "\n",
    "df_dbf2['weighted_income_median_avg']=df_dbf2.apply(weighted_income, axis=1)\n",
    "df_dbf2['weighted_age_median_avg']=df_dbf2.apply(weighted_age, axis=1)\n",
    "df_dbf2['medianinco']=df_dbf2.apply(median_income, axis=1)\n",
    "df_dbf2['medianage']=df_dbf2.apply(median_age, axis=1)\n",
    "df_dbf2['labour_coef']=df_dbf2.apply(labour_calc, axis=1)\n",
    "df_dbf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "print(color.BOLD + 'Minimum amounts' + color.END)\n",
    "print(df_dbf2[['medianinco','medianage']].min())\n",
    "print(color.BOLD + '\\nAggregated weighted average info' + color.END)\n",
    "print(df_dbf2[['poptot','weighted_income_median_avg','weighted_age_median_avg']].sum())\n",
    "print(color.BOLD + '\\nMaximum amounts' + color.END)\n",
    "print(df_dbf2[['medianinco','medianage']].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the information above and wikipedia averages for NYC overall and boroughs, I've manually selected linear transformation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbf2['medianinco']=df_dbf2.loc[:,'medianinco'].apply(lambda x: 30000+x/36)\n",
    "df_dbf2['medianage']=df_dbf2.loc[:,'medianage'].apply(lambda x: 30+x/100)\n",
    "\n",
    "df_dbf2['weighted_income_median_avg']=df_dbf2.apply(weighted_income, axis=1)\n",
    "df_dbf2['weighted_age_median_avg']=df_dbf2.apply(weighted_age, axis=1)\n",
    "\n",
    "print(color.BOLD + 'Minimum amounts' + color.END)\n",
    "print(df_dbf2[['medianinco','medianage']].min())\n",
    "print(color.BOLD + '\\nAggregated weighted average info' + color.END)\n",
    "print(df_dbf2[['poptot','weighted_income_median_avg','weighted_age_median_avg']].sum())\n",
    "print(color.BOLD + '\\nMaximum amounts' + color.END)\n",
    "print(df_dbf2[['medianinco','medianage']].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbf2=df_dbf2.loc[:,['medianinco','medianage','popinlabou','labour_coef']]\n",
    "df_dbf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:#this thing is needed only to run this cell without needing previous iterations\n",
    "    df_merged.drop(['medianinco','medianage','labour_coef'],axis=1,inplace=True)\n",
    "except:\n",
    "    print('No need to delete columns')\n",
    "\n",
    "try:#this thing the same as above\n",
    "    df_merged.set_index('NTA Code',inplace=True)\n",
    "except:\n",
    "    print('No need to set index')\n",
    "\n",
    "df_merged=df_merged.join(df_dbf2)\n",
    "df_merged.reset_index(inplace=True)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please note, that there is a confirmed error with Google Chrome browser showing folium maps - that there is a problem displaying 80+ elements on the map - it just shows emplty maps\n",
    "# also there is an example - 'https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/GeoJSON_and_choropleth.ipynb?flush_cache=true'\n",
    "# where you can use GeoJson instead of choropleth, however for this task choropleth and mozilla combination works fine\n",
    "\n",
    "# define the world map\n",
    "world_map = folium.Map(location=[40.7128,-74.0060], zoom_start=12)\n",
    "#please be aware that location=[latitude, longitude] is encoded in folium, however GEOjson files have [longitude,latitude] written down\n",
    "folium.Choropleth(\n",
    "     name='Population density layer',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=df_merged,\n",
    "     columns=['NTA Code', 'Population_density'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='YlOrRd', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Population density by NTA'\n",
    ").add_to(world_map)\n",
    "\n",
    "folium.Choropleth(\n",
    "     name='Median income layer',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=df_merged,\n",
    "     columns=['NTA Code', 'medianinco'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='BuGn', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Median income amount by NTA'\n",
    ").add_to(world_map)\n",
    "\n",
    "folium.Choropleth(\n",
    "     name='Median age layer',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=df_merged,\n",
    "     columns=['NTA Code', 'medianage'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='RdPu', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Median age by NTA'\n",
    ").add_to(world_map)\n",
    "\n",
    "folium.Choropleth(\n",
    "     name='Working people percentage',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=df_merged,\n",
    "     columns=['NTA Code', 'labour_coef'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='RdBu', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Share of working population'\n",
    ").add_to(world_map)\n",
    "\n",
    "\n",
    "folium.LayerControl().add_to(world_map)\n",
    "\n",
    "world_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please keep in mind that the following map has layers - switch through them in order to see various stats on the map\n",
    "We also see that the median age and median income look very much alike - sp I think it would be wise to drop the age parameter - because similar, or correlated parameters are usually bad for model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm saving this file as csv on local computer in order to have the step 1 end-result ready without running the whole programm\n",
    "df_merged.to_csv(r'NYC_current_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - extraction of Foursquare data\n",
    "\n",
    "## So now we have all external data sorted out, it is the time for us to get the foursquare data to be extracted - we're going to go through all the neighborhoods and extract all the venues in the max radius amount\n",
    "Limit is going to be calculated based on the radius amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'FICA04YHTU00Y1DHOH1T5VALLE2VGNRGTJJBVB0ECZ4QEE1C' # your Foursquare ID\n",
    "CLIENT_SECRET = 'LFO0GLMBQQQTCB1TCBSDFMKLRDGGEQK4O1VW5YLN4SR1NPHK' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "#first we read some data\n",
    "df_merged=pd.read_csv('NYC_current_data.csv')\n",
    "df_merged.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_merged['geometry']=df_merged.loc[:,'geometry'].apply(lambda x:  shapely.wkt.loads(x))\n",
    "df_merged['Shape centroid']=df_merged.loc[:,'Shape centroid'].apply(lambda x: ast.literal_eval(x))\n",
    "df_merged['Max radius, km']=df_merged.loc[:,'Max radius, km'].apply(lambda x: ast.literal_eval(x))\n",
    "df_merged.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function doesn't take\n",
    "def getNearbyVenues(row_series):\n",
    "    url='https://api.foursquare.com/v2/venues/search'\n",
    "    NTA_Code=row_series[0]\n",
    "    shapely_point=row_series[1]\n",
    "    max_radius=float(row_series[2])*1000\n",
    "    NTA_geometry=row_series[3]\n",
    "    lng=round(shapely_point[0],6)\n",
    "    lat=round(shapely_point[1],6)\n",
    "    params={\n",
    "            'client_id':CLIENT_ID,\n",
    "            'client_secret':CLIENT_SECRET,\n",
    "            'v':VERSION,\n",
    "            'intent':'browse',\n",
    "            'll':str(lat)+','+str(lng),\n",
    "            'limit':300, # in accordance with their API, limit is 50, however, let's try our luck\n",
    "            'radius':int(max_radius) #convert km into meters\n",
    "    }\n",
    "    # make the GET request\n",
    "    results = requests.get(url,params=params).json()['response']['venues']\n",
    "    # return only relevant information for each nearby venue\n",
    "    \n",
    "    venues_list=[]\n",
    "    for v in results:\n",
    "        if not v['categories']:\n",
    "            category_id=np.NaN\n",
    "            category_name=np.NaN\n",
    "        else:\n",
    "            category_id=v['categories'][0]['id']\n",
    "            category_name=v['categories'][0]['name']\n",
    "        cur_tuple=(\n",
    "            NTA_Code,\n",
    "            NTA_geometry,\n",
    "            shapely_point,\n",
    "            v['name'], \n",
    "            v['location']['lat'],\n",
    "            v['location']['lng'],\n",
    "            category_id,\n",
    "            category_name\n",
    "        )\n",
    "        venues_list.append(cur_tuple)\n",
    "\n",
    "    nearby_venues = pd.DataFrame(venues_list)\n",
    "    nearby_venues.columns = ['NTA Code', \n",
    "                  'geometry',\n",
    "                  'Shape centroid',\n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude',\n",
    "                  'Venue Category ID',\n",
    "                  'Venue Category Name']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell had been created solely for testing purposes - in order not to spend all my API limit on debugging\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "test_NTA_Code=df_merged.loc[0,['NTA Code','Shape centroid','Max radius, km','geometry']]\n",
    "#getNearbyVenues(test_NTA_Code)\n",
    "testmulti_NTA_Code=pd.DataFrame(zip(df_merged.loc[6,'Shape centroid'],df_merged.loc[6,'Max radius, km'],df_merged.loc[6,'geometry']),columns=['Shape centroid','Max radius, km','geometry'])\n",
    "testmulti_NTA_Code['NTA Code']=df_merged.loc[6,'NTA Code']\n",
    "testmulti_NTA_Code=testmulti_NTA_Code[['NTA Code','Shape centroid','Max radius, km','geometry']]\n",
    "for index_test, row_test in testmulti_NTA_Code.iterrows():\n",
    "    if index_test==0:\n",
    "        print(row_test)\n",
    "        df_curr=getNearbyVenues(row_test)\n",
    "    else:\n",
    "        df_curr=df_curr.append(getNearbyVenues(row_test),ignore_index=True)\n",
    "df_curr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_elem, row_elem in df_merged.iterrows():\n",
    "    if row_elem['Poly Count']==1:\n",
    "        df_curr=getNearbyVenues(row_elem[['NTA Code','Shape centroid','Max radius, km','geometry']])\n",
    "    else:\n",
    "        multi_row_split=pd.DataFrame(zip(row_elem['Shape centroid'],\n",
    "                                         row_elem['Max radius, km'],\n",
    "                                         row_elem['geometry']),\n",
    "                                     columns=['Shape centroid','Max radius, km','geometry'])\n",
    "        multi_row_split['NTA Code']=row_elem['NTA Code']\n",
    "        multi_row_split=multi_row_split[['NTA Code','Shape centroid','Max radius, km','geometry']]\n",
    "        for index_single,row_single in multi_row_split.iterrows():\n",
    "            if index_single==0:\n",
    "                df_curr=getNearbyVenues(row_single)\n",
    "            else:\n",
    "                df_curr=df_curr.append(getNearbyVenues(row_single),ignore_index=True)\n",
    "    if index_elem==0:\n",
    "        df_venues=df_curr\n",
    "    else:\n",
    "        df_venues=df_venues.append(df_curr,ignore_index=True)\n",
    "df_venues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll save the extracted data and finish this part - because we have API requests limit on foursquare\n",
    "df_venues.to_csv(r'NYC_venues_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - transformaton of extracted from Foursquare data\n",
    "\n",
    "## The next step is to confirm that these venues really inside the shapely objects, because the request had been formed to find all the venues in a circle with the center in the Polygon centroids (multiple circles for multipolygons) and radius determined as the farthest distance to the polygon element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues=pd.read_csv('NYC_venues_list.csv')\n",
    "df_venues.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_venues['geometry']=df_venues.loc[:,'geometry'].apply(lambda x:  shapely.wkt.loads(x))\n",
    "df_venues['Shape centroid']=df_venues.loc[:,'Shape centroid'].apply(lambda x: ast.literal_eval(x))\n",
    "# let's delete not venues without categories\n",
    "df_venues.dropna(inplace=True)\n",
    "df_venues.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues.loc[df_venues['NTA Code']=='BX09'].head() # so this means that the multipolygon neighborhoods got inserted a distinct polygon inside that shape attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this cell, we're removing all the objects, that actually don't belong to the neighborhood - due to the foursquare cirle extraction\n",
    "def condition_contain (row):\n",
    "    try: \n",
    "        return row['geometry'].contains(shapely.geometry.Point(row['Venue Longitude'],row['Venue Latitude']))\n",
    "    except:\n",
    "        print(row['geometry'])\n",
    "        print(row['Venue Longitude'])\n",
    "        print(row['Venue Latitude'])\n",
    "        return False\n",
    "\n",
    "df_venues['condition']=df_venues.apply(condition_contain,axis=1)\n",
    "df_venues=df_venues.loc[df_venues['condition']==True].drop('condition',axis=1).reset_index()\n",
    "df_venues.tail()\n",
    "#as you can see, we've deleted about 2000 Venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's aggregate similar venue categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'FICA04YHTU00Y1DHOH1T5VALLE2VGNRGTJJBVB0ECZ4QEE1C' # your Foursquare ID\n",
    "CLIENT_SECRET = 'LFO0GLMBQQQTCB1TCBSDFMKLRDGGEQK4O1VW5YLN4SR1NPHK' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "def recursion_cat_search(json_file,prev_list=[]):\n",
    "    curr_list=prev_list+[(json_file['id'],json_file['name'])]\n",
    "    df_curr=pd.DataFrame()\n",
    "    if not json_file['categories']:\n",
    "        curr_dict={}\n",
    "        for i in range(len(curr_list)):\n",
    "            curr_dict['category {}'.format(i)]=curr_list[i]\n",
    "        df_curr=df_curr.append(curr_dict,ignore_index=True,sort=False)\n",
    "        return df_curr\n",
    "    else:\n",
    "        for item in json_file['categories']:\n",
    "            df_curr=df_curr.append(recursion_cat_search(item,prev_list=curr_list),sort=False)\n",
    "    return df_curr\n",
    "cat_url='https://api.foursquare.com/v2/venues/categories'\n",
    "params={\n",
    "        'client_id':CLIENT_ID,\n",
    "        'client_secret':CLIENT_SECRET,\n",
    "        'v':VERSION,\n",
    "}\n",
    "cat_response=requests.get(cat_url,params=params).json()['response']\n",
    "df_venue_cats=pd.DataFrame()\n",
    "for cats in cat_response['categories']:\n",
    "    df_venue_cats=df_venue_cats.append(recursion_cat_search(cats),sort=False)\n",
    "df_venue_cats=df_venue_cats.reset_index()\n",
    "df_venue_cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def id_extractor(elem):\n",
    "    if type(elem) is not tuple:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return elem[0]\n",
    "# we need to modify this categories, to see only general and end-points - below code is old and not applicable\n",
    "# def end_cat_finder(row):\n",
    "#     if type(row['category 4']) is not tuple:\n",
    "#         if type(row['category 3']) is not tuple:\n",
    "#             if type(row['category 2']) is not tuple:\n",
    "#                 if type(row['category 1']) is not tuple:\n",
    "#                     return row['category 0']\n",
    "#                 else:\n",
    "#                     return row['category 1']\n",
    "#             else:\n",
    "#                 return row['category 2']\n",
    "#         else:\n",
    "#             return row['category 3']\n",
    "#     else:\n",
    "#         return row['category 4']\n",
    "\n",
    "# df_venue_cats['category_final']=df_venue_cats.apply(end_cat_finder,axis=1)\n",
    "# df_venue_cats['category ID']=df_venue_cats['category_final'].apply(lambda x: x[0])\n",
    "\n",
    "df_venue_cats['General Category ID']=df_venue_cats['category 0'].apply(lambda x: x[0])\n",
    "df_venue_cats['General Category name']=df_venue_cats['category 0'].apply(lambda x: x[1])\n",
    "df_venue_cats['cat_0_ID']=df_venue_cats['category 0'].apply(id_extractor)\n",
    "df_venue_cats['cat_1_ID']=df_venue_cats['category 1'].apply(id_extractor)\n",
    "df_venue_cats['cat_2_ID']=df_venue_cats['category 2'].apply(id_extractor)\n",
    "df_venue_cats['cat_3_ID']=df_venue_cats['category 3'].apply(id_extractor)\n",
    "df_venue_cats['cat_4_ID']=df_venue_cats['category 4'].apply(id_extractor)\n",
    "df_venue_cats.drop(['index','category 0','category 1','category 2','category 3','category 4'],axis=1,inplace=True)\n",
    "df_venue_cats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venue_cats[['General Category ID','General Category name','cat_4_ID']].dropna().set_index('cat_4_ID')\n",
    "\n",
    "for i in range(5):\n",
    "    cat_word='cat_{}_ID'.format(i)\n",
    "    curr_cat=df_venue_cats[['General Category ID','General Category name',cat_word]].dropna().drop_duplicates().set_index(cat_word)\n",
    "    print(curr_cat.shape)\n",
    "    if i==0:\n",
    "        df_venues_new=df_venues.join(curr_cat,on='Venue Category ID',how='inner')\n",
    "    else:\n",
    "        df_venues_new=df_venues_new.append(df_venues.join(curr_cat,on='Venue Category ID',how='inner'),ignore_index=True,sort=False)\n",
    "    print(df_venues_new.shape)\n",
    "df_venues_new.drop('index',inplace=True,axis=1)\n",
    "df_venues_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of San Francisco\n",
    "NYC_food_venues=df_venues_new[df_venues_new['General Category name']=='Food']\n",
    "NYC_food_map = folium.Map(location = [40.7128,-74.0060], zoom_start = 12)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "Venue_loc = plugins.MarkerCluster().add_to(NYC_food_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label, in zip(NYC_food_venues['Venue Latitude'], NYC_food_venues['Venue Longitude'], NYC_food_venues['Venue Category Name']):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=label,\n",
    "    ).add_to(Venue_loc)\n",
    "\n",
    "# display map\n",
    "NYC_food_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's aggregate the different categories (generalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_venues_new[['NTA Code','General Category name','Venue']].groupby(by=['NTA Code','General Category name']).count()\n",
    "\n",
    "NYC_onehot = pd.get_dummies(df_venues_new[['General Category name']], prefix=\"\", prefix_sep=\"\")\n",
    "NYC_onehot['NTA Code']=df_venues_new['NTA Code']\n",
    "Cols=NYC_onehot.columns.to_list()\n",
    "Pos=Cols.index('NTA Code')\n",
    "Cols=[Cols[Pos]]+Cols[:Pos]+Cols[Pos+1:]\n",
    "NYC_onehot=NYC_onehot[Cols]\n",
    "NYC_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_venue_grouped = NYC_onehot.groupby('NTA Code').sum().reset_index()\n",
    "print(NYC_venue_grouped.shape)\n",
    "NYC_venue_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_map = folium.Map(location=[40.7128,-74.0060], zoom_start=12)\n",
    "#please be aware that location=[latitude, longitude] is encoded in folium, however GEOjson files have [longitude,latitude] written down\n",
    "folium.Choropleth(\n",
    "     name='College & University',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=NYC_venue_grouped,\n",
    "     columns=['NTA Code', 'College & University'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='YlOrRd', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Number of Coll/Universities'\n",
    ").add_to(world_map)\n",
    "\n",
    "folium.Choropleth(\n",
    "     name='Food',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=NYC_venue_grouped,\n",
    "     columns=['NTA Code', 'Food'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='BuGn', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Number of Food joints'\n",
    ").add_to(world_map)\n",
    "\n",
    "# folium.Choropleth(\n",
    "#      name='Nightlife Spot',\n",
    "#      geo_data=json_geoinfo,\n",
    "#      data=NYC_venue_grouped,\n",
    "#      columns=['NTA Code', 'Nightlife Spot'],\n",
    "#      key_on='feature.properties.NTACode',\n",
    "#      fill_color='RdPu', \n",
    "#      fill_opacity=0.7, \n",
    "#      line_opacity=0.2,\n",
    "#      legend_name='Number of Nightlife spots'\n",
    "# ).add_to(world_map)\n",
    "\n",
    "# folium.Choropleth(\n",
    "#      name='Shop & Service',\n",
    "#      geo_data=json_geoinfo,\n",
    "#      data=NYC_venue_grouped,\n",
    "#      columns=['NTA Code', 'Shop & Service'],\n",
    "#      key_on='feature.properties.NTACode',\n",
    "#      fill_color='YlOrRd', \n",
    "#      fill_opacity=0.7, \n",
    "#      line_opacity=0.2,\n",
    "#      legend_name='Number of shops and services'\n",
    "# ).add_to(world_map)\n",
    "\n",
    "# folium.Choropleth(\n",
    "#      name='Professional & Other Places',\n",
    "#      geo_data=json_geoinfo,\n",
    "#      data=NYC_venue_grouped,\n",
    "#      columns=['NTA Code', 'Professional & Other Places'],\n",
    "#      key_on='feature.properties.NTACode',\n",
    "#      fill_color='BuGn', \n",
    "#      fill_opacity=0.7, \n",
    "#      line_opacity=0.2,\n",
    "#      legend_name='Number of professional & other places'\n",
    "# ).add_to(world_map)\n",
    "\n",
    "# folium.Choropleth(\n",
    "#      name='Travel & Transport',\n",
    "#      geo_data=json_geoinfo,\n",
    "#      data=NYC_venue_grouped,\n",
    "#      columns=['NTA Code', 'Travel & Transport'],\n",
    "#      key_on='feature.properties.NTACode',\n",
    "#      fill_color='RdBu', \n",
    "#      fill_opacity=0.7, \n",
    "#      line_opacity=0.2,\n",
    "#      legend_name='Number of travel & transport places'\n",
    "# ).add_to(world_map)\n",
    "\n",
    "folium.LayerControl().add_to(world_map)\n",
    "\n",
    "world_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart - merging venue and other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=pd.read_csv('NYC_current_data.csv')\n",
    "df_merged.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_final=df_merged.join(NYC_venue_grouped.set_index('NTA Code'),on='NTA Code')\n",
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(r'NYC_stat_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we read some data\n",
    "df_final=pd.read_csv('NYC_stat_aggregated.csv').set_index('NTA Code')\n",
    "df_final.drop(['Unnamed: 0','Year','Max radius, km','FIPS County Code','Shape type','Poly Count','geometry','Shape centroid','Event'],axis=1,inplace=True)\n",
    "df_final_clean=df_final.dropna()\n",
    "print(df_final_clean.shape,df_final.shape)\n",
    "df_final_clean.head()\n",
    "# so if we delete all the rows without some of the data, we'll lose 23 NTA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_data=df_final_clean[['Food']]\n",
    "x_data=df_final_clean.drop(['Borough','NTA Name','Food'],axis=1)\n",
    "#x_data=df_final_clean.drop(['NTA Code','Borough','NTA Name','Food'],axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=linear_model.LinearRegression()\n",
    "\n",
    "x_train_select=x_train[['Area_normalized, sq km']]\n",
    "x_test_select=x_test[['Area_normalized, sq km']]\n",
    "lr.fit(x_train_select,y_train)\n",
    "np.set_printoptions(precision=4,formatter={\"float_kind\": lambda x: \"%g\" % x})\n",
    "print(np.around(lr.coef_,decimals=4))\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(x_train_select, lr.coef_[0][0]*x_train_select + lr.intercept_[0], '-r')\n",
    "plt.scatter(df_final_clean['Area_normalized, sq km'], df_final_clean['Food'], color='blue')\n",
    "plt.xlabel(\"Area_normalized, sq km\")\n",
    "plt.ylabel(\"Food Venues count\")\n",
    "print('R^2 for the test data is {}'.format(lr.score(x_test_select, y_test)))\n",
    "plt.show()\n",
    "#as you can see, NTA area size doesn't show any significant effect on the Food venues count, and R^2 error is somewhere on the floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_select=x_train[['Population']]\n",
    "x_test_select=x_test[['Population']]\n",
    "lr.fit(x_train_select,y_train)\n",
    "np.set_printoptions(precision=4,formatter={\"float_kind\": lambda x: \"%g\" % x})\n",
    "print(np.around(lr.coef_,decimals=4))\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(x_train_select, lr.coef_[0][0]*x_train_select + lr.intercept_[0], '-r')\n",
    "plt.scatter(df_final_clean['Population'], df_final_clean['Food'], color='blue')\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Food Venues count\")\n",
    "print('R^2 for the test data is {}'.format(lr.score(x_test_select, y_test)))\n",
    "plt.show()\n",
    "#as you can see, NTA population doesn't show any significant effect on the Food venues count, and R^2 error is also very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_select=x_train[['popinlabou']]\n",
    "x_test_select=x_test[['popinlabou']]\n",
    "lr.fit(x_train_select,y_train)\n",
    "np.set_printoptions(precision=4,formatter={\"float_kind\": lambda x: \"%g\" % x})\n",
    "print(np.around(lr.coef_,decimals=4))\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(x_train_select, lr.coef_[0][0]*x_train_select + lr.intercept_[0], '-r')\n",
    "plt.scatter(df_final_clean['popinlabou'], df_final_clean['Food'], color='blue')\n",
    "plt.xlabel(\"Median income amount\")\n",
    "plt.ylabel(\"Food Venues count\")\n",
    "print('R^2 for the test data is {}'.format(lr.score(x_test_select, y_test)))\n",
    "plt.show()\n",
    "# well, for population that is working, we have a 0.08 R squared, which I will include in our model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_select=x_train[['Shop & Service']]\n",
    "x_test_select=x_test[['Shop & Service']]\n",
    "lr.fit(x_train_select,y_train)\n",
    "np.set_printoptions(precision=4,formatter={\"float_kind\": lambda x: \"%g\" % x})\n",
    "print(np.around(lr.coef_,decimals=4))\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(x_train_select, lr.coef_[0][0]*x_train_select + lr.intercept_[0], '-r')\n",
    "plt.scatter(df_final_clean['Shop & Service'], df_final_clean['Food'], color='blue')\n",
    "plt.xlabel(\"Shop & Services venue count\")\n",
    "plt.ylabel(\"Food Venues count\")\n",
    "print('R^2 for the test data is {}'.format(lr.score(x_test_select, y_test)))\n",
    "plt.show()\n",
    "# well, for population that is working, we have a almost 0.5 R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try linear models with x as 'Nightlife Spot' and 'Shop & Service'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_data=df_final[['Food']]\n",
    "x_data=df_final[['Nightlife Spot']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "\n",
    "lr.fit(x_train,y_train)\n",
    "print(\"The test R squared is equal to\", lr.score(x_test,y_test), 'mean squared error is equal to',mean_squared_error(y_test,lr.predict(x_test)))\n",
    "pd.DataFrame(lr.coef_,columns=x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data=df_final[['Food']]\n",
    "x_data=df_final[['Shop & Service']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "\n",
    "lr.fit(x_train,y_train)\n",
    "print(\"The test R squared is equal to\", lr.score(x_test,y_test), 'mean squared error is equal to',mean_squared_error(y_test,lr.predict(x_test)))\n",
    "pd.DataFrame(lr.coef_,columns=x_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a multilinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data=df_final[['Food']]\n",
    "x_data=df_final[['popinlabou','Arts & Entertainment','College & University','Nightlife Spot','Outdoors & Recreation','Professional & Other Places','Residence','Shop & Service','Travel & Transport']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "\n",
    "lr.fit(x_train,y_train)\n",
    "print(\"The test R squared is equal to\", lr.score(x_test,y_test), 'mean squared error is equal to',mean_squared_error(y_test,lr.predict(x_test)))\n",
    "pd.DataFrame(lr.coef_,columns=x_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pr=PolynomialFeatures(degree=2)\n",
    "x_train_pr=pr.fit_transform(x_train)\n",
    "x_test_pr=pr.fit_transform(x_test)\n",
    "\n",
    "RidgeModel=Ridge(alpha=200000000) # experimenting with various degree and alpha values this looks like the optimal solution\n",
    "RidgeModel.fit(x_train_pr, y_train)\n",
    "print(\"The train R squared is equal to\", RidgeModel.score(x_train_pr,y_train), 'mean squared error is equal to',mean_squared_error(y_train,RidgeModel.predict(x_train_pr)))\n",
    "print(\"The test R squared is equal to\", RidgeModel.score(x_test_pr,y_test), 'mean squared error is equal to',mean_squared_error(y_test,RidgeModel.predict(x_test_pr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And as the last model, let's try to do K-fold for the multilinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# since we've deleted all the unessesary data, we can restore 20 deleted NTA's with NaN values\n",
    "#x_data=df_final_clean.drop(['NTA Code','Borough','NTA Name','Food'],axis=1)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)\n",
    "#lr.fit(x_train,y_train)\n",
    "#print('R^2 for the test data is {}'.format(lr.score(x_test, y_test)))\n",
    "#pd.DataFrame(np.around(lr.coef_,decimals=4),columns=x_train.columns)\n",
    "Rcross = cross_val_score(lr, x_data, y_data, cv=4)\n",
    "yhat=cross_val_predict(lr,x_data, y_data,cv=4)\n",
    "print(\"The mean of the folds are\", Rcross.mean(), \"and the mean squared error is\" , mean_squared_error(y_data,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the K-fold model shown the best result! let's apply it to the whole dataset to determine which NTA is the most undersaturated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_func(row):\n",
    "    return row['Food venues estimate']-row['Food']\n",
    "\n",
    "df_compare=df_final[['NTA Name','Borough','Food']]\n",
    "df_compare.loc[:,'Food venues estimate']=cross_val_predict(lr,x_data, y_data,cv=4)\n",
    "df_compare.loc[:,'Food Venue count diff']=df_compare.apply(minus_func,axis=1)\n",
    "df_compare.sort_values('Food Venue count diff',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So this cross-validated multiple linear regression concludes our task of seing the most undersaturated restaurant Neighborhood tabulation areas in New-york city\n",
    "### As a bonus, let's build a map using the results that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_map = folium.Map(location=[40.7128,-74.0060], zoom_start=12)\n",
    "#please be aware that location=[latitude, longitude] is encoded in folium, however GEOjson files have [longitude,latitude] written down\n",
    "df_compare.reset_index(inplace=True)\n",
    "folium.Choropleth(\n",
    "     name='Food Venues prediction differences',\n",
    "     geo_data=json_geoinfo,\n",
    "     data=df_compare,\n",
    "     columns=['NTA Code', 'Food Venue count diff'],\n",
    "     key_on='feature.properties.NTACode',\n",
    "     fill_color='YlOrRd', \n",
    "     fill_opacity=0.7, \n",
    "     line_opacity=0.2,\n",
    "     legend_name='Difference between predicted and actual Food Venue Count'\n",
    ").add_to(world_map)\n",
    "\n",
    "world_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
